from sklearn.utils.extmath import randomized_svd
from scipy.sparse.linalg import svds
from math import isclose
import numpy as np

def svd(m,**kwargs):
    nc = kwargs.get('n_components', 1)
    ni = kwargs.get('n_iter', 10) 
    rs = kwargs.get('random_state', 42)
    return randomized_svd(m, n_components=nc, n_iter=ni, random_state=rs)

def loss(B,Y,mask):
    return 0.5 * np.sum(np.power(B[mask] - Y[mask],2))

def gradient(B,Y,mask):
    delta_B = (B-Y)
    delta_B[np.logical_not(mask)] = 0
    return delta_B

def general_gradient(B_k_1,Y,mask,t,t_k):
    return (B_k_1 - projection(B_k_1 - t_k * gradient(B_k_1, Y, mask),t,r)) /t_k

def frobenius_norm(m):
    return np.sum(np.power(m,2))

def trace_norm(m):
    r,c = m.shape
    #print("trace norm dimension: {}.".format(min(r,c)))
    _, s, _ = svd(m, n_components=min(r,c))
    #print(s)
    return np.sum(s)

def subgradient_op(B,Y,mask,t):
    u, _, vt = svd(gradient(B,Y,mask))
    sub = -t * u @ vt
    return sub

def frank_wolfe_iteration(B,Y,mask,t,gamma):
    S = subgradient_op(B,Y,mask,t)
    B_next = (1-gamma) * B + gamma * S
    return B_next

def frank_wolfe(B,Y,mask,t,tol=0,max_iter=15000):
    B_k_1 = B
    losses = [loss(B, Y, mask)]
    k = 0
    #print(trace_norm(B_k_1))
    while True:
        k += 1
        gamma = 2/(k+1) 
        B_k = frank_wolfe_iteration(B_k_1,Y,mask,t,gamma)
        #print(trace_norm(B_k))
        losses.append(loss(B_k,Y,mask))
        if np.abs(losses[-1] - losses[-2]) <= tol or k >= max_iter:
            break
        else:
            B_k_1 = B_k
    return B_k, np.array(losses)

def projection(B,t,r):
    u, s, vt = svd(B,n_components=1)
    n = min(B.shape[0],B.shape[1])
    B_tmp = B - u @ np.diag(s) @ vt
    for i in range(2,n+1):
        u_tmp, s_tmp, vt_tmp = svds(B_tmp,k=1)
        B_tmp = B_tmp - u_tmp @ np.diag(s_tmp) @ vt_tmp
        theta = (np.sum(s) + s_tmp[0] - t)/i
        print(i)
        print(len(s))
        print(theta)
        print(s_tmp)
        if  theta >= s_tmp[0]:
            break
        else:
            u = np.concatenate((u,u_tmp),axis=1) 
            vt = np.concatenate((vt,vt_tmp), axis=0)
            s = np.concatenate((s_tmp,s))
    print(s-theta)
    s = s - theta
    #negative_all = s < 0
    #while not isclose(t,np.sum((s-theta).clip(0))) and np.sum((s-theta).clip(0)) > t:
    #    tmp = s-theta
    #    negative_new = np.logical_xor(tmp < 0, negative_all)
    #    positive_all = tmp > 0
    #    inc = np.sum(tmp[negative_new])
    #    theta -= inc/np.sum(positive_all)
    #    negative_all = np.logical_or(negative_new, negative_all)
    #s = (s-theta).clip(0)
    return u @ np.diag(s) @ vt

def projected_gradient_descent(B,Y,mask,t,r,tol=0,use_backtracking=False,max_iter=15000):
    B_k_1 = B
    losses = [loss(B, Y, mask)]
    t_k = 1
    k = 0
    while True:
        k += 1
        if use_backtracking:
            t_k = backtracking(B_k_1,Y,mask,delta_B)
            print(alpha_k)
        B_k = projection(B_k_1 - t_k * gradient(B_k_1, Y, mask),t,r)
        losses.append(loss(B_k,Y,mask))
        if frobenius_norm(delta_B) <= tol or k >= max_iter:
            break
        else:
            B_k_1 = B_k
    return B_k, np.array(losses)

def find_lambda(B_k, B_k_1, Y, mask, lambda_max = 1e30, lambda_min = 1e-30):
    S_k_1 = B_k - B_k_1
    Y_k_1 = gradient(B_k,Y,mask) - gradient(B_k_1,Y,mask)
    if np.trace(S_k_1.T @ Y_k_1) == 0:
        return lambda_min
    lambda_k = np.trace(S_k_1.T @ S_k_1)/np.trace(S_k_1.T @ Y_k_1)
    if lambda_k > lambda_max:
        return lambda_max
    elif lambda_k < lambda_min:
        return lambda_min
    else:
        return lambda_k

def backtracking(B,Y,mask,d_k,beta=0.8):
    t_k = 1
    while True:
        #print(alpha)
        if loss(B+alpha*d_k,Y,mask) <= loss(B,Y,mask) - 0.5 * alpha * np.trace(d_k.T@d_k): 
            break
        else:
            alpha = alpha * beta
    return alpha

def projected_Barzilai_Borwein_method(B,Y,mask,t,r,tol=1e-6,L=10,beta=0.1,alpha_max=1e30,alpha_min=1e-30,sigma=0.01,max_iter=15000):
    f_best = loss(B,Y,mask)
    f_c = f_best
    f_r = np.Inf
    alpha_k = 1
    k = 0
    losses = [f_best]
    B_k_1 = B
    while True:
        k += 1
        d = projection(B_k_1 - alpha_k * gradient(B_k_1, Y, mask),t,r) - B_k_1
        lambda_k = 1
        B_k = B_k_1 + lambda_k * d
        print("while")
        while loss(B_k,Y,mask) > f_r + sigma * lambda_k * np.trace(gradient(B_k_1, Y, mask).T @ d):
            print(loss(B_k,Y,mask))
            print(f_r + sigma * lambda_k * np.trace(gradient(B_k_1, Y, mask).T @ d))
            print(f_r)
            print(lambda_k)
            lambda_k = lambda_k * beta
            B_k = B_k + lambda_k * d
        print("end while")
        g_B_k = gradient(B_k,Y,mask)
        g_B_k_1 = gradient(B_k_1,Y,mask)
        Y_k = g_B_k - g_B_k_1
        S_k = B_k - B_k_1
        if frobenius_norm(projection(g_B_k,t,r)) > frobenius_norm(projection(g_B_k_1,t,r)):
            alpha_k = np.trace(S_k.T @ Y_k)/np.trace(Y_k.T @ Y_k)
        else:
            alpha_k = np.trace(S_k.T @ S_k)/np.trace(S_k.T @ Y_k)
        alpha_k = min(max(alpha_k,alpha_min),alpha_max)
        f_curr = loss(B_k,Y,mask)
        losses.append(f_curr)
        if f_curr < f_best:
            f_best = f_curr
            f_c = f_curr
            l = 0
        else:
            f_c = max(f_c, f_curr)
            l = l + 1
            if l == L:
                f_r = f_c
                f_c = f_curr
                l = 0
        if loss(B_k, B_k_1, mask) <= tol:
            break
        else:
            B_k_1 = B_k
        if k >= max_iter:
            break
    return B_k, losses
